````markdown
# DataProcess: Serverless Event-Driven ETL Pipeline on AWS

> **Project:** Automated, serverless data ingestion and processing infrastructure.
> **Stack:** Terraform (IaC), Python, AWS (Step Functions, Lambda, EventBridge, Aurora).

---

## 1. Context & Objectives

This project addresses the needs of a Data company ("DataProcess") requiring a reliable and scalable solution to ingest, clean, and consolidate large JSON files.

### Key Challenges Solved
* **Complex Orchestration:** Managing dependent tasks (Job A and B must finish before Job C starts).
* **Fully Automated:** Workflow triggers instantly upon file upload (Event-Driven).
* **Network Security:** Strict isolation of the database within a Private VPC.
* **Cost Optimization:** 100% Serverless architecture ("Pay-as-you-go").

---

## 2. Technical Architecture

The pipeline relies on a **State Machine** model to ensure resilience and error handling.

### Data Flow
1.  **Ingestion:** User uploads a JSON file to the input S3 bucket (via a secure Presigned URL generated by API Gateway).
2.  **Trigger:** `Amazon EventBridge` detects the `ObjectCreated` event and triggers the State Machine.
3.  **Orchestration (Step Functions):**
    * **Parallel State:** Concurrent execution of **Enrichment** and **Anonymization** functions.
    * **Synchronization:** Waits for both processes to complete successfully.
    * **Consolidation:** Aggregates results using a third Lambda function.
4.  **Storage:** Final data is written to `Amazon Aurora PostgreSQL` (hosted in a private subnet).

![Workflow Architecture](proof_parallel_workflow_2.png)

---

## 3. Project Structure (Infrastructure as Code)

The entire infrastructure is provisioned using **Terraform**, ensuring reproducibility.

```text
.
├── main.tf             # Provider and Backend configuration
├── vpc.tf              # Network (VPC, Private Subnets, VPC Endpoints)
├── security.tf         # Security (Security Groups, IAM Roles, PoLP Policies)
├── compute.tf          # Compute (Lambda Functions, VPC Config)
├── database.tf         # Data (RDS PostgreSQL / Aurora)
├── events.tf           # Event-Driven (EventBridge Rules, Targets)
├── workflow.asl.json   # State Machine Definition (Amazon States Language)
└── src/                # Python Source Code for Lambdas
````

-----

## 4\. Deployment Validation (Proof of Concept)

### A. Infrastructure Provisioning

All resources (Database, Buckets, Lambdas) were successfully provisioned via Terraform.

### B. Automated Trigger (Event-Driven)

Uploading a file to the input S3 bucket instantly triggered a new execution in Step Functions via EventBridge.

**Before Upload:**

**After Upload (File Received):**

**Immediate Result (Execution Started):**

### C. Parallel Orchestration Success

The Step Functions graph view confirms that Enrichment and Anonymization steps ran in parallel, followed by Consolidation, validating the workflow logic.

-----

## 5\. Security & Architectural Choices

| Component | Security / Architectural Decision |
| :--- | :--- |
| **Lambda-in-VPC** | The Consolidation function is deployed within the Private VPC to access RDS securely without traversing the public internet. |
| **VPC Endpoints** | Used Gateway Endpoints to allow private Lambdas to access S3 without a NAT Gateway (Reducing costs and attack surface). |
| **IAM Least Privilege** | Each service (EventBridge, Step Functions, Lambda) has a dedicated IAM Role with minimal required permissions. |
| **Presigned URLs** | (API Module) Uploads are performed via temporary signed URLs to avoid exposing the bucket with public write access. |

-----

## 6\. How to Deploy

```bash
# 1. Initialize Terraform
terraform init

# 2. Review the plan
terraform plan

# 3. Deploy
terraform apply -auto-approve
```

-----

*Project realized as part of the AWS Solutions Architect Associate (SAA-C03) certification path.*

```
```